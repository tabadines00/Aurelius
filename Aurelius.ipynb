{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Add Your API Keys here"
      ],
      "metadata": {
        "id": "cSSpcpFBwSkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openaikey = \"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\"\n",
        "pineconekey = \"bdc66999-e03f-4530-9b43-ce20beb42793\"\n",
        "pineconeenv = \"gcp-starter\""
      ],
      "metadata": {
        "id": "wXBL8Tv5wR0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "Vb8OSJ6rceBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR14v1exu5XW",
        "outputId": "50630b0a-82f9-4bcd-d0c3-ed573c5cb3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain openai pinecone-client datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "Z4msXtCLqU8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_vEQ7uKtPA9",
        "outputId": "f3f5886f-34af-4e34-d250-ffa81e82d0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the data and setting up Pinecone"
      ],
      "metadata": {
        "id": "-AKq6B-6cuBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the cleaned text file of Meditations into the TextLoader."
      ],
      "metadata": {
        "id": "X7XomSH1zuRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"./meditations-cleaned.txt\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "5fIz01EYrrzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the text file into chunks to seperate sentences into small groups. If we make the chunks too large, they might contain too many sentences with too many differing ideas. If we make the chunks too small,the relationships between the ideas may not be clear."
      ],
      "metadata": {
        "id": "WeqD8mXKzyKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "q6g_nxgprCuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[242]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmy2q0uTr0S-",
        "outputId": "390f2036-0f0a-4411-e47b-f52ced544d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"Where the matter may be effected agreeably to that reason, which both unto the Gods and men is common, there can be no just cause of grief or sorrow. For where the fruit and benefit of an action well begun and prosecuted according to the proper constitution of man may be reaped and obtained, or is sure and certain, it is against reason that any damage should there be suspected. In all places, and at all times, it is in thy power religiously to embrace whatsoever by God's appointment is happened unto thee, and justly to converse with those men, whom thou hast to do with, and accurately to examine every fancy that presents itself, that nothing may slip and steal in, before thou hast rightly apprehended the true nature of it.\", metadata={'source': './meditations-cleaned.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Pinecone index if it is not already created."
      ],
      "metadata": {
        "id": "P_jhiq17w1Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'langchain-retrieval-augmentation'\n",
        "pinecone.init(\n",
        "    api_key=pineconekey,\n",
        "    environment=pineconeenv\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ],
      "metadata": {
        "id": "7QpV4MxnwFxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne_b_w6-wy2z",
        "outputId": "84db0707-94ed-4406-f72e-4f1cc4d39074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 0}},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Embeddings using OpenAI's Ada embedding model. These embeddings will group the similar text chunks together, with the hopes that similar ideas and topics will have similar vector embeddings. The more related the ideas, the closer the embeddings will be to each other."
      ],
      "metadata": {
        "id": "9jx9Kpj5v11N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=openaikey\n",
        ")"
      ],
      "metadata": {
        "id": "cFUnlv5gv0Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embed, index_name=index_name)"
      ],
      "metadata": {
        "id": "iY8yb6xctc70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we query the Vector database using a Cosine Similarity Search, which finds the most similar chunks of text by measuring the cosine distance between the vector embeddings in our database."
      ],
      "metadata": {
        "id": "Z3Kv5aVM3NV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "4F9zU9IzyVVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.8, openai_api_key=openaikey)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "sDn5WuvDyf2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will \"stuff\" the query to GPT with the similar chunks found in the Pinecone database in order to give the model context from Marcus' writings and writing style."
      ],
      "metadata": {
        "id": "-n-AfqvVmcd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What do I do if I am feeling lonely?\"\n",
        "docs = docsearch.similarity_search(query, k=3, include_metadata=False)"
      ],
      "metadata": {
        "id": "SuceGlwouysh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AFL5SNN5o7GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "systemprompt = query + \" Using these attached excerpts of his writing, answer as if you were Marcus Aurelius giving me advice. Write using his antiquated style.\"\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "33jR3jq0y4fO",
        "outputId": "119bc61a-13f4-4db9-ad8b-db7ebc960842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Remind yourself that it is only the present moment that can hurt you, and that moment can be reduced if you focus on small moments of patience. Try to recognize the positives in your current situation, and remind yourself that other people are available to help when needed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syIcaQWF53vE",
        "outputId": "12ef50f2-58fa-4e08-a221-13e473322c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='whensoever thou must use the help of others.', metadata={}),\n",
              " Document(page_content='lest I also in time forget myself.', metadata={}),\n",
              " Document(page_content='Let not the general representation unto thyself of the wretchedness of this our mortal life, trouble thee. Let not thy mind wander up and down, and heap together in her thoughts the many troubles and grievous calamities which thou art as subject unto as any other. But as everything in particular doth happen, put this question unto thyself, and say: What is it that in this present matter, seems unto thee so intolerable? For thou wilt be ashamed to confess it. Then upon this presently call to mind, that neither that which is future, nor that which is past can hurt thee; but that only which is present. (And that also is much lessened, if thou dost lightly circumscribe it:) and then check thy mind if for so little a while, (a mere instant), it cannot hold out with patience.', metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}